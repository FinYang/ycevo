---
title: "US yield curve 2007: Reproducing Koo, La Vecchia, & Linton (2019)"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{US yield curve 2007: Reproducing Koo, La Vecchia, & Linton (2019)}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---
  
```{r, include = FALSE}
knitr::opts_chunk$set(
collapse = TRUE,
comment = "#>", 
eval = FALSE
)
```


## Introduction

The goal of ycevo is to provide a range of functions to facilitate the non-parametric estimation of the discount rate, and yield curve, of CRSP Bond Data.

If you use any data or code from the `ycevo` package in a publication, please use the following citation:  

> Bonsoo Koo, Nathaniel Tomasetti, Kai-Yang Goh and Yangzhuoran Yang (2019). ycevo: Non-Parametric Estimation
of the Yield Curve Evolution. R package version 1.0.0. https://github.com/bonsook/ycevo. 

The package provides code used in Koo, La Vecchia, & Linton (2019). Please use the following citation if you use any result from the paper.

> Koo, B., La Vecchia, D., & Linton, O. B. (2019). Estimation of a Nonparametric model for Bond Prices from Cross-section and Time series Information. Available at SSRN3341344.

This vignette aims to provide steps and documentation for producing part of the results (tables, figures, etc) in Koo, La Vecchia, & Linton (2019). For a simple simulation example using the package, refer to vignette _Introduction to ycevo: Simulation example_.


## Reproducing paper

Load in the libraries we will use.
```{r, message = FALSE, eval = TRUE}
library(ycevo)
library(tidyverse)
library(lubridate)
library(plotly)
library(akima)
```

The data must be provided as a data.frame or tibble object, with one row corresponding to each payment - either the coupon or principal - for one bond on one quotation date.
The functions require particular column names:

* qdate: The quotation date, as an integer or lubridate::date object

* crspid: Some character or factor that uniquely identifies the bond

* mid.price: The quoted price of the bond on that day

* tupq: Time until a given payment, usually given in days

* tumat: Time until maturity, in days (equal to tupq for zero coupon bonds)

* pdint: Numeric value of payment

* accint: The accumulated interest on payments

For example, if a bond traded on 18/9/2019 has five payments remaining until maturity, it should have five rows in the data with a quotation date of 18/9/2019. Each of these five rows will have a different value of pdint and tupq, corresponding to the five payments, but will have the same value for price, accint, and tumat

The CRSP bond database for 2007 is provided as USbonds as an example
```{r}
USbonds %>% 
  filter(year==2007) %>% 
  glimpse()
```



The daily three month treasury bill rate from 1954 to 2018 is provided, it's called DTB3. 
```{r}
glimpse(DTB3)
```

There are two variables:

1. date, the current day.

2. rate, the three month treasury bill interest rate on that day.

First we need to define our `ugrid`, the grid of quotation dates that we want to estimate the discount rate for. `ugrid` is provided as a decimal from 0 to 1. A `ugrid` of 0 indicates the first available day in the data, and a `ugrid` of 1 indicates the last available day. Each `ugrid` value is associated with a bandwidth. Quotation dates within `ugrid` $\pm$ `hu` will be included in the calculations, with the epaker kernel function. 
It works best if the intervals defined by `ugrid` $\pm$ `hu` do not include zero or one. We could simply use the following as it is only one year.


```{r, eval = FALSE}
# Not run
ugrid <- c(0.2, 0.4, 0.6, 0.8)
hu <- rep(0.2, 4)
```

The data used in Koo, La Vecchia, & Linton (2019) covers the seven-year-period from Jan. 2001 to Dec. 2007. To reproduce the result in the paper, instead of using the simple `ugrid` as shown above, we retrieve the `ugrid` used in the paper, which is calculated over a seven-year-period.

```{r}
years <- 2001:2007
ugridPerYear <- c(seq(0.2, 0.8, 0.2), rep(seq(0, 0.8, 0.2), length(years) - 1))
ugrid <- seq(0.2, length(years) - 0.2, 0.2) / length(years)
year <- rep(years, c(4, rep(5, length(years) - 1)))
hu <- rep(0.2/length(years), 5*length(years) - 1)
```


The following was the interest rates for this period. We will define `rgrid` and `hr` as the interest rate grid and bandwidth. Make sure the interest rate is within `rgrid` $\pm$ `hr` for long enough in each ugrid window or we may not have enough data. We use deterministic time and three-month treasury bill rates `treasure$rate` as factors.

```{r, fig.cap="Three-month Treasury bill rates (2007)", fig.align="center"}
treasury <- DTB3
data <- USbonds
dates <- unique(data$qdate)

interest <- treasury %>%
  filter(date %in% dates)

ggplot(interest) + geom_line(aes(date, rate))
```


```{r}
window_day <- calc_day_idx(USbonds, ugrid, hu)
uRange <- dates[window_day]
uRange <- data.frame(minDate = uRange[1:34], maxDate = uRange[35:68])

int <- lapply(seq_len(nrow(uRange)), function(i) filter(interest, date >= uRange$minDate[i] & date <= uRange$maxDate[i]))
rgrid <- lapply(int, function(int) quantile(int$rate, c(1/3, 2/3)))
hr <- lapply(seq_along(rgrid), function(i)
  1.5 * c(max(rgrid[[i]][1] - min(int[[i]]$rate), rgrid[[i]][2] - rgrid[[i]][1]),
          max(max(int[[i]]$rate) - rgrid[[i]][2], rgrid[[i]][2] - rgrid[[i]][1]))  )
```

The interest grid is entirely optional, and we can simply ignore the rgrid, hr, and interest arguments in all of our functions. 



Next we need a grid of the time-to-payment values that we want to estimate a discount rate for. This grid is referred to as `qgrid`. Some of the functions used below have a units argument. If qgrid is in the same unit as tupq is (e.g., days), set units to 1. If tupq is in days and you want to specify qgrid in years, set units to 365. Each value of qgrid plus/minus hq defines a bin. It is okay if there is no observed payments for some of these bins (we will interpolate nearby payments) but the first and last bin needs to contain some payment. We include a few lines here to check the latest bond in the data, and truncate qgrid to this value.

There's lots of data for short term maturities, and less data for longer maturities. We will let `qgrid` change as the duration increases.

```{r, eval = FALSE}
# Not run
qgrid <- c(seq(30, 6 * 30, 30),  # Monthly up to six months
           seq(240, 2 * 365, 60),  # Two months up to two years
           seq(2 * 365 + 90, 5 * 365, 90),  # Three months up to five years
           seq(5 * 365+ 120, 10 * 365, 120),  # four months up to ten years
           seq(10 * 365 + 365, 35 * 365, 365)) / 365 # Annually up to 35 years
max_tumat <- max(data$tumat)
if(max_tumat/365 < max(qgrid)){
  qgrid <- qgrid[1:min(which(qgrid >= max_tumat/365))]
}

```

We want to calculate the slists over the entire range of ugrid values even if there are some zero interest rate grids, so we calculate the qgrid in each iteration for different ugrid and rgrid in the loop.


In the loop, we also need to create a bandwidth for each of these qgrid values. We will use `max(qgrid - dplyr::lag(qgrid), dplyr::lead(qgrid) - qgrid)`.
```{r, eval= FALSE}
# Not run
# We set hq to be the maximum of the gap between a given qgrid and the previous value, and that qgrid and the next value
laggap <- qgrid - dplyr::lag(qgrid)
leadgap <- dplyr::lead(qgrid) - qgrid
hq <- vapply(1:length(qgrid), function(x) max(laggap[x], leadgap[x], na.rm = TRUE), numeric(1))
```

Data of this type often leaves large gaps in the time-to-payment grid where we have insufficient data to estimate the discount rate. We use a sparse grid of time values, `xgrid`, where we are sure we have enough data for stable estimates. This is done via the `create_xgrid_hx` function, which has a `min_points` argument. This function inputs `qgrid`, as well as our other grids, `ugrid` and optionally `rgrid`. It creates an `xgrid` that has values of `qgrid` that have at least `min_points` many maturing bonds appearing in that window, and discards the other points. The function outputs a list with named values `xgrid` and `hx`. `hx` is defined by default to be the same as `hq`, but is increased whenever we have omitted an `xgrid` value from `qgrid.` You may use your own `xgrid` and `hx` as you like, but we have found this approach works well for the CRSP data. This usually happens for longer time-to-payment values, above around 15 years.

Unfortunately the strategy used by `create_xgrid_hx` usually means that the size of `xgrid` changes for different values of `ugrid` and `rgrid`. The functions don't support `xgrid` with differing lengths, so we loop through the `rgrid` and `ugrid` values and input one combination at a time. Combining all the above together, we have the following loop.


```{r}
dhat <- data.frame()
# To get the result only for 2007, we use only the last 4 ugrids
for(i in tail(seq_along(ugrid), 4)){

  for(j in seq_along(rgrid[[i]])){
    print(paste(i, j))
    # Ensure that only qdates inside uu_window are passed onto the create_xgrid_hx function
    windowU <- calc_uu_window(data, ugrid[i], hu[i])
    windowR <- calc_r_window(interest$rate, rgrid[[i]][j], hr[[i]][j])
    window <- windowU * windowR
    if(sum(window != 0) < 5){
      next
    }
    uKernel <- data.frame(qdate = dates, k = window, ku = windowU)
    dataK <- left_join(data, uKernel, by = 'qdate')

    # We want to calculate the slists over the entire range of ugrid values even if there are some zero interest rate grids
    # This is why there is 'ku', the u kernel and 'k', the u & r kernel
    # Later on only data where 'k' is non-zero will be used, but the gaps in the kernel cause problems with the slists
    cf_slist <- vector(mode = "list", length = length(dates))
    cf_slist[which(windowU != 0)] <- calc_cf_slist(dataK[dataK$ku != 0, ])

    price_slist <- vector(mode = "list", length = length(dates))
    price_slist[which(windowU != 0)] <-  calc_price_slist(dataK[dataK$ku != 0, ])

    # Stop qgrid after the maximum maturity date.
    max_tumat <- max(dataK[dataK$k != 0, ]$tumat)

    qgrid <- c(seq(30, 6 * 30, 30),  # Monthly up to six months
               seq(240, 2 * 365, 60),  # Two months up to two years
               seq(720 + 90, 6 * 365, 90),  # Three months up to six years
               seq(2160 + 120, 20 * 365, 120),  # Four months up to 20 years
               #               seq(20 * 365 + 182, 30 * 365, 182)) / 365 # Six months up to 30 years
               seq(20 * 365 + 182, 30.6 * 365, 182)) / 365
    # Cut qgrid so that there is one value greater than the maximum to maturity
    qgrid <- qgrid[1:min(which(qgrid >= max_tumat / 365))]

    laggap <- qgrid - dplyr::lag(qgrid)
    leadgap <- dplyr::lead(qgrid) - qgrid
    hq <- vapply(1:length(qgrid), function(x) max(laggap[x], leadgap[x], na.rm = TRUE), numeric(1))

    grids <- create_xgrid_hx(dataK[dataK$k != 0, ], ugrid[i], hu[i], qgrid, hq, 5)

    res <- estimate_yield(data, ugrid[i], hu[i], rgrid[[i]][j], hr[[i]][j],
                          grids$xgrid, grids$hx, grids$qgrid, grids$hq, price_slist, cf_slist, interest$rate,
                          loess = FALSE)
    dhat <- rbind(dhat, res)
  }

}

ug <- data.frame(ug = ugrid, ugpy = ugridPerYear, year = year)
dhat <- dhat %>%
  left_join(ug)
```





Now we have a discount rate (and yield) on a three-dimensional grid of values. There are many values that appear in the data that we do not have a discount rate for, so we interpolate the results to get a discount rate on any quotation date (`qdate` / `ugrid`), time to payment (`tupq` / `xgrid`) and interest rate (`rgrid`). We expect that discount rates are non-linear over `xgrid`, so we use loess to interpolate this. The other grids have linear interpolation. The function `interpolate_dhat` does all of this for us. If you didn't include `rgrid`, you can omit the `treasury` argument, but if you did include `rgrid`, and the resulting dataframe `dhat` includes the corresponding `rg` column, you will need `treasury`.

```{r}
smoothed <- data.frame()
day_grid <- dhat %>% select(ug, rg) %>% unique()
for(i in 1:nrow(day_grid)){
  res <- filter(dhat, ug == day_grid$ug[i] & rg == day_grid$rg[i])
  # Polynomial interpolation
  loess_fit <- loess(discount ~ qg, data = res)
  smoothed <- rbind(smoothed, data.frame(qg= loess_fit$x, discount = loess_fit$fitted, ug = day_grid$ug[i], rg = day_grid$rg[i]))
}
# Converting back to 0.2, 0.4 etc.
smoothed  %>%
  left_join(ug) %>%
  mutate(yield = -log(discount) / qg) -> smoothed
```


We then can get the (forecast) residual using the following code.

```{r}
# Get unique values of x
# Add an extra year to the dataset to be forecasted as fcDates may include January of the next year.
dataFC <- rbind(data, US_list[[max(years) - 1999]])

interpX <- dataFC %>%
  ungroup() %>%
  #  filter(tupq > min(qgrid) * 365) %>%
  mutate(x = as.numeric(tupq) / 365) %>%
  select(x) %>%
  distinct() %>%
  .$x
interp <- data.frame()
for(i in 1:nrow(day_grid)){
  res <- filter(dhat, ug == day_grid$ug[i] & rg == day_grid$rg[i])
  # Polynomial interpolation
  loess_fit <- loess(discount ~ qg, data = res)
  interp <- rbind(interp,
                  data.frame(x = interpX, dhat = predict(loess_fit, interpX), ug = day_grid$ug[i], rg = day_grid$rg[i]))
}



# Forecasting
H <- 30
fcError <- NULL

# To get the result only for 2007, we use only the last 4 ugrids
for(i in tail(seq_along(ugrid), 4)){

  # Check when current u window ends so we can forecast the next H days
  windowU <- calc_uu_window(data, ugrid[i], hu[i])
  end <- max(which(windowU != 0))
  fcDates <- dates[end] + 1:H

  forecast <- filter(dataFC, qdate %in% fcDates)

  # Build interest rate model for past one year of interest rate data
  interestSub <- filter(treasury, date <= dates[end] & date > (dates[end] - years(1)))
  interestMod <- forecast::auto.arima(interestSub$rate)
  interestFC <- data.frame(qdate = fcDates,
                           fcRate = forecast::forecast(interestMod, H)$mean,
                           h = 1:H,
                           ugrid = ugrid[i])
  forecast <- left_join(forecast, interestFC)

  # Get interpolated dhat values (across time to maturity) for this ugrid
  interpMat <- interp %>%
    filter(ug == ugrid[i]) %>%
    select(x, dhat, rg) %>%
    spread(rg, dhat) %>% as.matrix()
  rg <- filter(day_grid, ug == ugrid[i])$rg
  #interpolate these over rgrid, get forecast error
  error <- forecast %>%
    mutate(x = as.numeric(tupq) / 365) %>%
    group_by(qdate, crspid, matdate, mid.price, accint, x) %>%
    mutate(dhat = as.numeric((interpolate_ugrid(x, fcRate, rg, interpMat, x))),
           price = mid.price + as.numeric(as.character(accint))) %>%
    ungroup() %>%
    group_by(qdate, crspid, price, tumat, couprt, h, fcRate, ugrid) %>%
    summarise(phat = sum(pdint * dhat)) %>%
    mutate(ferror = price - phat,
           year = lubridate::year(dates[end]))
  fcError <- rbind(fcError, error)
}


# Set up for error interpolation

error_data <- data %>%
  ungroup() %>%
  mutate(x = as.numeric(tupq) / 365,
         qdate = factor(qdate, labels = dates),
         u = as.numeric(qdate) / length(dates),
         qdate = date(qdate)) %>%
  left_join(interest %>% select(date, rate),
            by = c('qdate'= 'date'))

# To get the result only for 2007, we use only the last 4 ugrids
day_grid$type <-  rep(1:2, 4)

rgrid <- day_grid %>%
  spread(ug, rg) %>%
  select(-type) %>%
  as.matrix()

# To get the result only for 2007, we use only the last 4 ugrids
interpCube <- array(0, dim = c(length(interpX), 4, 2))

ii <- 1
# To get the result only for 2007, we use only the last 4 ugrids
for(i in tail(seq_along(ugrid), 4)){
  interpCube[, ii, 1] <- interp[interp$ug == ugrid[i] & interp$rg == rgrid[1, ii], 'dhat']
  interpCube[, ii, 2] <- interp[interp$ug == ugrid[i] & interp$rg == rgrid[2, ii], 'dhat']
  ii <- ii+1
}

errors <- error_data %>%
  group_by(qdate, crspid, matdate, mid.price, accint, x, type) %>%
  mutate(dhat = as.numeric((interpolate_ugrid_rgrid(x, u, rate, tail(seq_along(ugrid), 5), t(rgrid), interpCube, interpX))), 
         price = mid.price + as.numeric(as.character(accint))) %>%
  ungroup() %>%
  group_by(qdate, crspid, price, tumat, couprt, type) %>%
  summarise(phat = sum(pdint * dhat)) %>%
  mutate(perror = price - phat,
         year = lubridate::year(qdate))



```

We can also calculate the variance. Because calculation of the variance only uses the rgrid, we use a for loop separately from the one before, but the two for loops follow the same logic.

```{r}
### Variances
variance <- data.frame()
# To get the result only for 2007, we use only the last 4 ugrids
for(i in tail(seq_along(ugrid), 4)){
  print(paste(i))
  # Ensure that only qdates inside uu_window are passed onto the create_xgrid_hx function
  windowU <- calc_uu_window(data, ugrid[i], hu[i])

  uKernel <- data.frame(qdate = dates, k = windowU)
  dataK <- left_join(data, uKernel, by = 'qdate')

  cf_slist <- calc_cf_slist(dataK[dataK$k != 0, ]) #[which(windowU != 0)]

  # Stop qgrid after the maximum maturity date.
  max_tumat <- max(dataK[dataK$k != 0, ]$tumat)

  qgrid <- c(seq(30, 6 * 30, 30),  # Monthly up to six months
             seq(240, 2 * 365, 60),  # Two months up to two years
             seq(720 + 90, 6 * 365, 90),  # Three months up to six years
             seq(2160 + 120, 20 * 365, 120),  # Four months up to 20 years
             seq(20 * 365 + 182, 35 * 365, 182)) / 365 # Six months up to 30 years
  # Cut qgrid so that there is one value greater than the maximum to maturity
  qgrid <- qgrid[1:min(which(qgrid >= max_tumat / 365))]

  laggap <- qgrid - lag(qgrid)
  leadgap <- lead(qgrid) - qgrid

  hq <- vapply(1:length(qgrid), function(x) max(laggap[x], leadgap[x], na.rm = TRUE), runif(1))
  grids <- select(dataK[dataK$k != 0, ], -k) %>%
    create_xgrid_hx(ugrid[i], hu[i], qgrid, hq, 5)

  var <- dhat_var(data = data, ugrid = ugrid[i], hu = hu[i],
                  xgrid = grids$xgrid, hx = grids$hx, perrors = errors[errors$qdate %in% uKernel[uKernel$k != 0, 'qdate'],],
                  dhat = smoothed, cf_slist = cf_slist)
  variance <- rbind(variance, var)
}
```

We now can produce some of the figures and tables in the paper. All the figures and tables we produce here only include 2007 results. Comparing to what is in the paper, they may be truncated. Note there may be possible difference from the paper, as the results in the paper are generated using the development version of the package. The difference in the way to count the dates in the development version from the current version may result in one or two days difference in the kernel bandwidth.

Due to the time limit, the result generated using the above code is stored in the data set `vignette_data` and extracted to produce the following figures and tables.


```{r, eval = TRUE}
attach(vignette_data)
```

```{r, eval = TRUE}
## ---- Figure-3 ----

interestUgrid <- data.frame()
g <- seq(0.2, 0.8, 0.2)
j <- 1
for(i in 31:34){
  int <- filter(interest, date >= uRange$minDate[i] & date <= uRange$maxDate[i])
  int$grid <- g[[j]]
  interestUgrid <- rbind(interestUgrid, int)
  j <- j+1
}

r <- rgrid[,2:5]
colnames(r) <- g
interestUgrid <- interestUgrid %>% 
  mutate(r1 = r[1, as.character(grid)], 
         r2 = r[2, as.character(grid)])
ggplot(interestUgrid) + 
  geom_line(aes(date, rate)) + 
  facet_wrap(~grid, scales = 'free_x') + 
  geom_line(aes(x = date, y = r1), linetype = 2)+
  geom_line(aes(x = date, y = r2), linetype = 2) +
  labs(x = "date", y = "3 month Treasury Bills rate")

## ---- Figure-4 ----
# Discount function graph with confidence interval
variance %>%
  filter(year == 2007) %>%
  mutate(lower = discount - 3*1.96 * sqrt(dhat_var),
         upper = discount + 3*1.96 * sqrt(dhat_var)) %>%
  drop_na() %>% 
  ggplot() + 
  geom_line(aes(qg, discount, group = rg)) +
  labs(x = "time to maturity (year)", y = "Discount") +
  geom_ribbon(aes(x =qg, ymin = lower, ymax = upper, group = rg), alpha = 0.3) +
  facet_wrap(year~ugpy, scales = 'free')

## ---- Figure-5 ----
#  Yield Curve graph with confidence interval
variance %>%
  filter(year == 2007) %>%
  mutate(lower = yield - 3*1.96 * sqrt(yield_var),
         upper = yield + 3*1.96 * sqrt(yield_var)) %>%
  drop_na() %>% 
  ggplot() + 
  geom_line(aes(qg, yield, group = rg)) + 
  labs(x = "time to maturity (year)", y = "Yield") +
  geom_ribbon(aes(x=qg, ymin = lower, ymax = upper, group = rg), alpha = 0.3) + 
  facet_wrap(year~ugpy, scales = 'free') +
  ylim(0,0.1)


## ---- Table-2 ----

reportVals <- variance %>%
  # filter(year == 2007) %>%
  mutate(
    mon1 = abs(qg - 1/12),
    mon3 = abs(qg - 0.25),
    mon6 = abs(qg - 0.5),
    year1 = abs(qg - 1),
    year3 = abs(qg - 3),
    year5 = abs(qg - 5),
    year10 = abs(qg - 10),
    year15 = abs(qg - 15)) %>%
  group_by(ug) %>%
  summarise(mon1 = qg[which.min(mon1)],
            mon3 = qg[which.min(mon3)],
            mon6 = qg[which.min(mon6)],
            year1 = qg[which.min(year1)],
            year3 =qg[which.min(year3)],
            year5 = qg[which.min(year5)],
            year10 =qg[which.min(year10)],
            year15 =qg[which.min(year15)]) %>%
  select(-ug)
# smoothed <- filter(smoothed, year == 2007)
table_discount <- NULL
ii <- 2
i <- 31
for(i in 31:34){
  temp <- variance %>%
    filter(rg == rgrid[1, ii], ug == ugrid[i] , qg %in% reportVals[ii-1, ]) 
  
  tempTable <- temp %>%
    mutate(discount = round(discount, 3),
           se = paste0('(', 2*round(sqrt(dhat_var), 3), ')')) %>%
    gather(est, value, discount, se) %>%
    group_by(qg) %>%
    mutate(time =  case_when(
      qg == reportVals[ii-1, 1] ~ '1 Month',
      qg == reportVals[ii-1, 2] ~ '3 Months',
      qg == reportVals[ii-1, 3] ~ '6 Months',
      qg == reportVals[ii-1, 4] ~ '1 Year',
      qg == reportVals[ii-1, 5] ~ '3 Years',
      qg == reportVals[ii-1, 6] ~ '5 Years',
      qg == reportVals[ii-1, 7] ~ '10 Years',
      qg == reportVals[ii-1, 8] ~ '15 Years',
      TRUE ~ 'qg'),
      time = factor(time, levels = c('1 Month', '3 Months', '6 Months', '1 Year', '3 Years', '5 Years', '10 Years', '15 Years')),
      ugrid = year + ugpy) %>%
    ungroup() %>%
    select(time, est, value, ugrid) %>%
    pivot_wider(names_from = time, values_from = value) %>% 
    select(-est)
  
  table_discount <- rbind(table_discount, tempTable)
  ii <- ii+1
}

# knitr::kable(table_discount, format = 'latex')
knitr::kable(table_discount)

## ---- Table-3 ----
# YIELD TABLE
table_yield <- NULL
ii <- 2
i <- 31
for(i in 31:34){
  temp <- variance %>%
    filter(rg == rgrid[1, ii]& ug == ugrid[i] & qg %in% reportVals[ii-1, ])
  
  tempTable <- temp %>%
    mutate(yield = round(yield, 3),
           yse = paste0('(', 2*round(sqrt(yield_var), 3), ')')) %>%
    gather(est, value, yield, yse) %>%
    group_by(qg) %>%
    mutate(time =  case_when(
      qg == reportVals[ii-1, 1] ~ '1 Month',
      qg == reportVals[ii-1, 2] ~ '3 Months',
      qg == reportVals[ii-1, 3] ~ '6 Months',
      qg == reportVals[ii-1, 4] ~ '1 Year',
      qg == reportVals[ii-1, 5] ~ '3 Years',
      qg == reportVals[ii-1, 6] ~ '5 Years',
      qg == reportVals[ii-1, 7] ~ '10 Years',
      qg == reportVals[ii-1, 8] ~ '15 Years',
      TRUE ~ 'qg'),
      time = factor(time, levels = c('1 Month', '3 Months', '6 Months', '1 Year', '3 Years', '5 Years', '10 Years', '15 Years')),
      ugrid = year + ugpy) %>%
    ungroup() %>%
    select(time, est, value, ugrid) %>%
    pivot_wider(names_from = time, values_from = value) %>% 
    select(-est) 
  table_yield <- rbind(table_yield, tempTable)
  ii <- ii+1
}

# knitr::kable(table_yield), format = 'latex')
knitr::kable(table_yield)
```

Due to the size limit of the vignette, we do not show the 3d charts but just to provide the code.

```{r, eval = TRUE}
## ---- Figure-6-a ----

plot_data <- mutate(variance, time = year + ugpy)%>%
  filter(time >= 2001 & time <= 2007.8) %>%
  #  filter(time >= "2001-01-01" & time <= "2007-12-31") %>%
  select(qg, time, discount, yield) %>%
  filter(qg >=0.5)
years <- 2007

time <- seq(0, 0.8, 0.05) # controls number of lines and how close they are
ttm <- seq(0.2, 18, 0.05)

time_grid <- years + time
```


```{r, eval = FALSE}
# discount function
dh <- akima::interp(x = plot_data$qg, y = plot_data$time, z = plot_data$discount,
                    xo = ttm, yo = time_grid, duplicate = "mean")
dh <- data.frame(x = rep(dh$x, length(dh$y)), y=rep(dh$y, each = length(dh$x)), z=c(dh$z))
plot_ly(dh, x = ~x, y = ~y, z = ~z, type = 'scatter3d', mode = 'lines') %>%
  layout(
    title = "Discount Function 3-dim",
    scene = list(
      xaxis = list(title = "time-to-maturity"),
      yaxis = list(title = "Year",
                   tickvals = time_grid,
                   ticktext = time_grid),
      zaxis = list(title = "Discount")
    ))

## ---- Figure-6-b ----
# yield curve
yh <- akima::interp(x = plot_data$qg, y = plot_data$time, z = plot_data$yield,
                    xo = ttm, yo = time_grid, duplicate = "mean")
yh <- data.frame(x = rep(yh$x, length(yh$y)), y=rep(yh$y, each = length(yh$x)), z=c(yh$z))
plot_ly(yh, x = ~x, y = ~y, z = ~z, type = 'scatter3d', mode = 'lines') %>%
  layout(
    title = "Discount Function 3-dim",
    scene = list(
      xaxis = list(title = "time-to-maturity"),
      yaxis = list(title = "Year",
                   tickvals = time_grid,
                   ticktext = time_grid),
      zaxis = list(title = "Yield")
    ))
```


```{r, eval = TRUE}
## ---- Figure-7-a ----
xVals <- unique(plot_data$qg)
TimeToMat <- 0.5
minqg <- xVals[which.min(abs(xVals - TimeToMat))]

plot_data %>%
  filter(qg == minqg) %>%
  filter(time >= 2004 & time <= 2007.9) %>%
  group_by(time) %>%
  mutate(yieldAv = mean(yield*100)) %>%
  ggplot() + geom_line(aes(time, yieldAv)) + labs(x = 'Time', y = 'Yield') + theme_bw()

## ---- Figure-7-c ----
### smoothing

xVals <- unique(plot_data$qg)
TimeToMat <- 5
minqg <- xVals[which.min(abs(xVals - TimeToMat))]

plot_data %>%
  filter(qg == minqg) %>%
  filter(time >= 2004 & time <= 2007.9) %>%
  group_by(time) %>%
  mutate(yieldAv = mean(yield*100)) %>%
  ggplot() + geom_line(aes(time, yieldAv)) + labs(x = 'Time', y = 'Yield') + theme_bw()
```



## License

This package is free and open source software, licensed under GPL-3
